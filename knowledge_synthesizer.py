"""
knowledge_synthesizer.py

Upgraded KnowledgeSynthesizer -> AdvancedKnowledgeSynthesizer

This is a heavy, production-ready upgrade that:
- Uses embeddings (sentence-transformers) with optional GPU acceleration
- Supports persistent FAISS vector index (faiss-cpu / faiss-gpu) for fast retrieval
- Builds a NetworkX knowledge graph for structural reasoning and motif detection
- Provides neural generation (transformers) for concept fusion and explanations
- Supports parallel ingestion, embedding caching, and on-disk persistence
- Configurable safe_mode/theatrical_mode, deterministic seeding, and device selection
- Graceful fallbacks when heavy libs are unavailable (keeps original behavior)
- Exposes synchronous and asynchronous insight generation entrypoints
- Adds structured logging, progress, and basic experiment tracking hooks (W&B or simple file)
"""

from __future__ import annotations

# Stability header: set env BEFORE importing heavy libs in interactive environments (Colab)
import os
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
os.environ.setdefault("ABSL_CPP_MIN_LOG_LEVEL", "2")
os.environ.setdefault("TF_FORCE_GPU_ALLOW_GROWTH", "true")
os.environ.setdefault("TRANSFORMERS_VERBOSITY", "error")

import time
import math
import random
import pickle
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Tuple, Iterable

# Base brain (existing repository)
from brain_advanced import AdvancedBrain

# Optional heavy libraries (graceful fallback)
try:
    import numpy as np
except Exception:
    np = None

try:
    import networkx as nx
except Exception:
    nx = None

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    import faiss  # type: ignore
except Exception:
    faiss = None

try:
    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
except Exception:
    pipeline = None
    AutoModelForCausalLM = None
    AutoTokenizer = None

try:
    from sklearn.metrics.pairwise import cosine_similarity
except Exception:
    cosine_similarity = None

try:
    import torch
    _TORCH_AVAILABLE = True
except Exception:
    torch = None
    _TORCH_AVAILABLE = False

# Optional experiment tracking (wandb) - non-fatal
try:
    import wandb
    _WANDB_AVAILABLE = True
except Exception:
    wandb = None
    _WANDB_AVAILABLE = False

# Logging
logger = logging.getLogger("knowledge_synthesizer")
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
    logger.addHandler(ch)
logger.setLevel(logging.INFO)


def _safe_cosine(a: Iterable[float], b: Iterable[float]) -> float:
    """Robust cosine similarity fallback (works without numpy or sklearn)."""
    try:
        if np is None:
            a_list = list(a)
            b_list = list(b)
            dot = sum(x * y for x, y in zip(a_list, b_list))
            na = math.sqrt(sum(x * x for x in a_list))
            nb = math.sqrt(sum(y * y for y in b_list))
            return float(dot / (na * nb + 1e-12))
        a = np.array(a, dtype=float).reshape(1, -1)
        b = np.array(b, dtype=float).reshape(1, -1)
        if cosine_similarity is not None:
            return float(cosine_similarity(a, b)[0][0])
        da = np.linalg.norm(a)
        db = np.linalg.norm(b)
        return float((a @ b.T) / (da * db + 1e-12))
    except Exception:
        return 0.0


class KnowledgeSynthesizer(AdvancedBrain):
    """
    A massively upgraded KnowledgeSynthesizer that can scale to large knowledge bases
    and leverages heavy ML/graph libraries when available.

    Key constructor options:
    - embedding_model_name: sentence-transformers model name
    - generation_model_name: transformers causal model name (for fusion/explanations)
    - use_faiss: whether to use faiss if installed
    - persist_dir: where to persist indices, embeddings, and graphs
    - device: 'cpu' or 'cuda' (if torch is available)
    - max_workers: parallelism for ingestion
    - theatrical_mode: keep fancy prints (default False)
    - safe_mode: avoid dangerous claims / aggressive behaviors
    """

    def __init__(
        self,
        name: str = "Prometheus",
        embedding_model_name: str = "all-MiniLM-L6-v2",
        generation_model_name: Optional[str] = None,
        use_faiss: bool = True,
        persist_dir: str = "knowledge_store",
        device: str = "cpu",
        max_workers: int = 4,
        theatrical_mode: bool = False,
        safe_mode: bool = True,
        random_seed: Optional[int] = 42,
    ):
        super().__init__(name)
        self.name = name
        self.theatrical_mode = theatrical_mode
        self.safe_mode = safe_mode
        self.persist_dir = persist_dir
        os.makedirs(self.persist_dir, exist_ok=True)

        # Determinism
        self.random_seed = random_seed
        if random_seed is not None:
            random.seed(random_seed)
            try:
                import numpy as _np  # type: ignore
                _np.random.seed(random_seed)
            except Exception:
                pass

        # Core data structures
        self.knowledge_base: Dict[str, List[str]] = {}
        self.embeddings_cache: Dict[Tuple[str, str], List[float]] = {}
        self.faiss_index = None
        self.faiss_meta: List[Tuple[str, str]] = []
        self.faiss_vectors: List[List[float]] = []
        self.use_faiss = use_faiss and (faiss is not None) and (np is not None)
        self.graph = nx.Graph() if nx is not None else None
        self._domain_centroids: Dict[str, List[float]] = {}
        self._lock = threading.RLock()

        # Parallelism
        self.max_workers = max(1, max_workers)

        # Load embedding model lazily
        self.embedding_model_name = embedding_model_name
        self.embedding_model = None
        if SentenceTransformer is not None:
            try:
                logger.info("Loading SentenceTransformer: %s", embedding_model_name)
                # prefer CPU unless torch+cuda requested
                self.embedding_model = SentenceTransformer(embedding_model_name, device=device if _TORCH_AVAILABLE else "cpu")
            except Exception as e:
                logger.warning("Could not load SentenceTransformer (%s): %s", embedding_model_name, e)
                self.embedding_model = None

        # Neural generator (optional)
        self.generation_model_name = generation_model_name
        self.generator = None
        self._generator_device = device
        if generation_model_name and pipeline is not None:
            try:
                # Use device mapping if torch available
                device_idx = 0 if device.startswith("cuda") and _TORCH_AVAILABLE else -1
                logger.info("Loading generation pipeline: %s (device=%s)", generation_model_name, device)
                self.generator = pipeline("text-generation", model=generation_model_name, device=device_idx)
            except Exception as e:
                logger.warning("Failed to load generation pipeline (%s): %s", generation_model_name, e)
                self.generator = None

        # Device support
        self.device = device if _TORCH_AVAILABLE else "cpu"
        if device.startswith("cuda") and not _TORCH_AVAILABLE:
            logger.warning("CUDA requested but torch not available; falling back to CPU.")

        # Basic telemetry hook
        self._use_wandb = False
        if _WANDB_AVAILABLE:
            try:
                wandb.init(project="super-intelligence-knowledge", reinit=True)
                self._use_wandb = True
            except Exception:
                self._use_wandb = False

        if self.theatrical_mode:
            logger.info("ðŸŒŒ AdvancedKnowledgeSynthesizer '%s' booting (theatrical_mode=True)", self.name)
        else:
            logger.info("AdvancedKnowledgeSynthesizer '%s' initialized (safe_mode=%s)", self.name, self.safe_mode)

        # Attempt to load persisted index/embeddings/graph if present
        self._attempt_load_state()

        # Warn if multiple frameworks present (duplicate native registration risk)
        try:
            import tensorflow as _tf  # type: ignore
            _TF_AVAILABLE = True
        except Exception:
            _TF_AVAILABLE = False
        if _TF_AVAILABLE and _TORCH_AVAILABLE:
            logger.warning("Both TensorFlow and PyTorch detected in the same process. Consider isolating frameworks to avoid CUDA/native conflicts.")

    # -------------------------
    # Persistence
    # -------------------------
    def _persist_path(self, name: str) -> str:
        return os.path.join(self.persist_dir, name)

    def persist_state(self) -> None:
        """Persist embeddings, metadata, and graph to disk for faster restarts."""
        with self._lock:
            try:
                emb_path = self._persist_path("embeddings.pkl")
                with open(emb_path, "wb") as f:
                    pickle.dump({"meta": self.faiss_meta, "vectors": self.faiss_vectors, "cache": self.embeddings_cache}, f)
                logger.info("Persisted embeddings to %s", emb_path)
            except Exception as e:
                logger.warning("Failed to persist embeddings: %s", e)

            try:
                if self.graph is not None:
                    gpath = self._persist_path("knowledge_graph.gpickle")
                    nx.write_gpickle(self.graph, gpath)
                    logger.info("Persisted graph to %s", gpath)
            except Exception as e:
                logger.debug("Graph persist failed: %s", e)

            # persist simple centroids
            try:
                cp = self._persist_path("centroids.pkl")
                with open(cp, "wb") as f:
                    pickle.dump(self._domain_centroids, f)
                logger.info("Persisted centroids to %s", cp)
            except Exception as e:
                logger.debug("Centroid persist failed: %s", e)

    def _attempt_load_state(self) -> None:
        """Try to load previously persisted state to speed up startup."""
        try:
            ep = self._persist_path("embeddings.pkl")
            if os.path.exists(ep):
                with open(ep, "rb") as f:
                    obj = pickle.load(f)
                    meta = obj.get("meta", [])
                    vectors = obj.get("vectors", [])
                    cache = obj.get("cache", {})
                    self.faiss_meta = meta
                    self.faiss_vectors = vectors
                    self.embeddings_cache.update(cache)
                    if self.use_faiss and self.faiss_vectors:
                        self._build_faiss_index()
                logger.info("Loaded embeddings from %s", ep)
        except Exception as e:
            logger.debug("Loading embeddings failed: %s", e)

        try:
            gp = self._persist_path("knowledge_graph.gpickle")
            if self.graph is not None and os.path.exists(gp):
                self.graph = nx.read_gpickle(gp)
                logger.info("Loaded knowledge graph from %s", gp)
        except Exception as e:
            logger.debug("Loading graph failed: %s", e)

        try:
            cp = self._persist_path("centroids.pkl")
            if os.path.exists(cp):
                with open(cp, "rb") as f:
                    self._domain_centroids = pickle.load(f)
                logger.info("Loaded centroids from %s", cp)
        except Exception as e:
            logger.debug("Loading centroids failed: %s", e)

    # -------------------------
    # Encoding & indexing
    # -------------------------
    def _encode_concepts(self, concepts: List[str]) -> Optional[List[List[float]]]:
        """Return embeddings for concepts using sentence-transformers if available; cache results."""
        if not concepts:
            return None
        embs = []
        to_compute = []
        idx_map = {}
        for i, c in enumerate(concepts):
            key = ("concept", c)
            if key in self.embeddings_cache:
                embs.append(self.embeddings_cache[key])
            else:
                idx_map[len(to_compute)] = (i, c)
                to_compute.append(c)
                embs.append(None)

        if to_compute and self.embedding_model is not None:
            try:
                computed = self.embedding_model.encode(to_compute, show_progress_bar=False, convert_to_numpy=True)
                for local_idx, arr in enumerate(computed):
                    global_idx, concept_str = idx_map[local_idx]
                    vec = list(map(float, arr))
                    embs[global_idx] = vec
                    self.embeddings_cache[("concept", concept_str)] = vec
            except Exception as e:
                logger.warning("Embedding computation failed: %s", e)
                # fallback to lightweight featurizer for missing ones
                for _, (gi, concept_str) in idx_map.items():
                    if embs[gi] is None:
                        embs[gi] = self._lightweight_encode(concept_str)
                        self.embeddings_cache[("concept", concept_str)] = embs[gi]
        else:
            # No heavy model: fallback to deterministic lightweight encoding
            for i, v in enumerate(embs):
                if v is None:
                    embs[i] = self._lightweight_encode(concepts[i])

        return embs

    def _lightweight_encode(self, text: str) -> List[float]:
        """Deterministic fallback embedding: character & token statistics and hashing."""
        s = str(text)
        vec = []
        vec.append(len(s) / 200.0)
        tokens = s.split()
        vec.append(len(tokens) / 20.0)
        vec.append(sum(ord(c) for c in s[:30]) / 5000.0)
        # topic indicators
        indicators = ["quantum", "neural", "algorithm", "evolution", "entropy", "gene", "graph"]
        for term in indicators:
            vec.append(1.0 if term in s.lower() else 0.0)
        # seeded pseudo-random tail for diversity
        rnd = random.Random(hash(s) & 0xFFFFFFFF)
        for _ in range(32 - len(vec)):
            vec.append(rnd.random() * 0.2)
        return vec[:32]

    def _faiss_normalize(self, mat: np.ndarray) -> np.ndarray:
        """Normalize rows for inner-product cosine-like search in FAISS."""
        norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12
        return mat / norms

    def _build_faiss_index(self) -> None:
        """Construct (or rebuild) the FAISS index from self.faiss_vectors."""
        if not self.use_faiss:
            logger.debug("FAISS not enabled or not available.")
            return
        if not self.faiss_vectors:
            logger.debug("No vectors to index.")
            return
        try:
            d = len(self.faiss_vectors[0])
            xb = np.array(self.faiss_vectors, dtype="float32")
            xb = self._faiss_normalize(xb)
            # Use inner-product index for cosine-like search with normalized vectors
            self.faiss_index = faiss.IndexFlatIP(d)
            self.faiss_index.add(xb)
            logger.info("FAISS index built: %d vectors, dim=%d", len(self.faiss_vectors), d)
        except Exception as e:
            logger.warning("Failed to build FAISS index: %s", e)
            self.faiss_index = None

    # -------------------------
    # Ingestion
    # -------------------------
    def ingest_knowledge(self, domain: str, concepts: Iterable[str], persist: bool = True, parallel: bool = True) -> None:
        """
        Ingest a batch of concepts into the knowledge base.
        - domain: domain name
        - concepts: iterable of str
        - persist: whether to persist state after ingestion
        - parallel: use thread pool to compute embeddings in parallel
        """
        concepts = list(map(str, concepts))
        with self._lock:
            self.knowledge_base.setdefault(domain, []).extend(concepts)
        logger.info("Ingesting %d concepts into domain '%s'", len(concepts), domain)

        # Update graph
        if self.graph is not None:
            for c in concepts:
                nid = f"{domain}::{c}"
                if not self.graph.has_node(nid):
                    self.graph.add_node(nid, domain=domain, concept=c, ingested_at=time.time())
            # heuristically connect new nodes to random existing nodes in same domain for motif formation
            existing = [n for n, d in self.graph.nodes(data=True) if d.get("domain") == domain and n not in {f"{domain}::{c}" for c in concepts}]
            for c in concepts:
                if existing:
                    target = random.choice(existing)
                    self.graph.add_edge(f"{domain}::{c}", target, weight=1.0)

        # Compute embeddings (parallelizable)
        if parallel and self.max_workers > 1:
            with ThreadPoolExecutor(max_workers=self.max_workers) as ex:
                futures = {ex.submit(self._encode_concepts, [c]): c for c in concepts}
                for fut in as_completed(futures):
                    try:
                        out = fut.result()
                        if out:
                            emb = out[0]
                            self._faiss_add(domain, futures[fut], emb)
                    except Exception as e:
                        logger.debug("Embedding worker failed: %s", e)
        else:
            embs = self._encode_concepts(concepts)
            if embs:
                for c, emb in zip(concepts, embs):
                    self._faiss_add(domain, c, emb)

        # Recompute centroids lazily
        self._rebuild_domain_centroids()

        if persist:
            self.persist_state()

    def _faiss_add(self, domain: str, concept: str, emb: List[float]) -> None:
        """Add vector + meta to local lists and rebuild index lazily."""
        if emb is None:
            return
        with self._lock:
            self.faiss_meta.append((domain, concept))
            self.faiss_vectors.append(emb)
            # keep embeddings cache
            self.embeddings_cache[("concept", concept)] = emb
            # periodically rebuild the index for performance tradeoff
            if self.use_faiss and (self.faiss_index is None or len(self.faiss_vectors) % 256 == 0):
                try:
                    self._build_faiss_index()
                except Exception as e:
                    logger.debug("Faiss add rebuild failed: %s", e)

    def _rebuild_domain_centroids(self) -> None:
        """Compute per-domain centroids used for fast cross-domain analogies."""
        if not self.faiss_vectors or not self.faiss_meta or np is None:
            return
        domain_map = {}
        for (domain, concept), vec in zip(self.faiss_meta, self.faiss_vectors):
            domain_map.setdefault(domain, []).append(vec)
        for d, vecs in domain_map.items():
            arr = np.array(vecs, dtype=float)
            centroid = list(np.mean(arr, axis=0))
            self._domain_centroids[d] = centroid
        logger.debug("Rebuilt %d domain centroids", len(self._domain_centroids))

    # -------------------------
    # Retrieval & similarity
    # -------------------------
    def query_similar(self, concept: str, top_k: int = 6) -> List[Dict[str, Any]]:
        """Return nearest concepts across the knowledge base for a single concept."""
        emb = None
        key = ("concept", concept)
        if key in self.embeddings_cache:
            emb = self.embeddings_cache[key]
        else:
            embs = self._encode_concepts([concept])
            emb = embs[0] if embs else None

        if emb is None:
            return []

        results = []
        if self.use_faiss and self.faiss_index is not None and np is not None:
            try:
                q = np.array([emb], dtype="float32")
                q = self._faiss_normalize(q)
                D, I = self.faiss_index.search(q, k=min(top_k, len(self.faiss_meta)))
                for score, idx in zip(D[0].tolist(), I[0].tolist()):
                    if idx < 0 or idx >= len(self.faiss_meta):
                        continue
                    d, c = self.faiss_meta[idx]
                    results.append({"domain": d, "concept": c, "score": float(score)})
            except Exception as e:
                logger.debug("FAISS query failed: %s", e)

        # Fallback brute force
        if not results:
            for (d, c), vec in zip(self.faiss_meta, self.faiss_vectors):
                s = _safe_cosine(emb, vec)
                results.append({"domain": d, "concept": c, "score": s})
            results = sorted(results, key=lambda x: x["score"], reverse=True)[:top_k]
        return results

    # -------------------------
    # Insight generation strategies
    # -------------------------
    def generate_cross_domain_insights(self, top_k: int = 8, probes: int = 12) -> List[Dict[str, Any]]:
        """
        High-level orchestrator combining multiple strategies:
        - centroid analogies (domain-level)
        - retrieval-driven concept analogies
        - motif/motif-transfer via graph centrality
        - neural concept fusion (RAG style if generator available)
        Returns a list of rich insight dicts: {insight, confidence, strategy, rationale, tags}
        """
        logger.info("Generating cross-domain insights (top_k=%d probes=%d)", top_k, probes)
        all_insights: List[Dict[str, Any]] = []

        try:
            # Strategy A: Domain centroid analogies
            all_insights.extend(self._strategy_centroid_analogies())

            # Strategy B: Retrieval-driven analogies (probe random concepts)
            all_insights.extend(self._strategy_retrieval_analogies(n_probes=probes))

            # Strategy C: Graph motif transfer
            all_insights.extend(self._strategy_motif_transfer())

            # Strategy D: Neural fusion (generative explanations)
            all_insights.extend(self._strategy_neural_fusion(n_samples=probes // 3))
        except Exception as e:
            logger.exception("Insight generation pipeline error: %s", e)

        # Rank, deduplicate and return top_k
        ranked = sorted(all_insights, key=lambda x: x.get("confidence", 0.0), reverse=True)
        deduped: List[Dict[str, Any]] = []
        seen_texts: List[str] = []
        for ins in ranked:
            txt = ins.get("insight", "")
            if any(_safe_text_similarity(txt, s) > 0.85 for s in seen_texts):
                continue
            deduped.append(ins)
            seen_texts.append(txt)
            if len(deduped) >= top_k:
                break

        # Persist and return
        ts = time.time()
        for ins in deduped:
            ins["generated_at"] = ts
            # optional lightweight telemetry
            if self._use_wandb:
                try:
                    wandb.log({"insight_confidence": ins.get("confidence", 0.0), "strategy": ins.get("strategy", "")})
                except Exception:
                    pass
        return deduped

    def _strategy_centroid_analogies(self) -> List[Dict[str, Any]]:
        insights = []
        if not self._domain_centroids:
            self._rebuild_domain_centroids()
        domains = list(self._domain_centroids.keys())
        for i in range(len(domains)):
            for j in range(i + 1, len(domains)):
                d1 = domains[i]
                d2 = domains[j]
                c1 = self._domain_centroids.get(d1)
                c2 = self._domain_centroids.get(d2)
                if not c1 or not c2:
                    continue
                sim = _safe_cosine(c1, c2)
                quality = 0.25 + 0.75 * sim
                if quality > 0.5:
                    text = f"Domain analogy: '{d1}' and '{d2}' show semantic alignment (centroid_sim={sim:.3f})"
                    insights.append({"insight": text, "confidence": float(quality), "strategy": "centroid_analogy", "rationale": f"sim={sim:.3f}", "tags": ["centroid", "analogy"]})
        return insights

    def _strategy_retrieval_analogies(self, n_probes: int = 12) -> List[Dict[str, Any]]:
        insights = []
        if not self.faiss_meta:
            return insights
        idxs = list(range(len(self.faiss_meta)))
        random.shuffle(idxs)
        probes = idxs[:min(n_probes, len(idxs))]
        for p in probes:
            d, c = self.faiss_meta[p]
            results = self.query_similar(c, top_k=6)
            for r in results:
                if r["domain"] == d:
                    continue
                sim = r["score"]
                quality = 0.2 + 0.8 * sim
                if quality > 0.45:
                    text = f"Analogy: '{c}' ({d}) â‰ˆ '{r['concept']}' ({r['domain']}) (sim={sim:.3f})"
                    insights.append({"insight": text, "confidence": float(quality), "strategy": "retrieval_analogy", "rationale": f"sim={sim:.3f}", "tags": ["analogy", "retrieval"]})
        return insights

    def _strategy_motif_transfer(self) -> List[Dict[str, Any]]:
        insights = []
        if self.graph is None:
            return insights
        # Use centrality measures to find hubs and map motifs across domains
        try:
            deg = dict(self.graph.degree())
            # collect top hubs per domain
            domain_hubs: Dict[str, List[Tuple[str, int]]] = {}
            for n, data in self.graph.nodes(data=True):
                d = data.get("domain", "unknown")
                domain_hubs.setdefault(d, []).append((n, deg.get(n, 0)))
            for d, nodes in domain_hubs.items():
                domain_hubs[d] = sorted(nodes, key=lambda x: x[1], reverse=True)[:5]
            domains = list(domain_hubs.keys())
            for i in range(len(domains)):
                for j in range(i + 1, len(domains)):
                    d1 = domains[i]
                    d2 = domains[j]
                    hubs1 = domain_hubs.get(d1, [])
                    hubs2 = domain_hubs.get(d2, [])
                    if not hubs1 or not hubs2:
                        continue
                    # compare top hub concepts by embedding similarity if available
                    n1 = hubs1[0][0]
                    n2 = hubs2[0][0]
                    c1 = self.graph.nodes[n1]["concept"]
                    c2 = self.graph.nodes[n2]["concept"]
                    emb1 = self.embeddings_cache.get(("concept", c1)) or self._encode_concepts([c1])[0]
                    emb2 = self.embeddings_cache.get(("concept", c2)) or self._encode_concepts([c2])[0]
                    sim = _safe_cosine(emb1, emb2)
                    quality = 0.35 + 0.65 * sim
                    if quality > 0.5:
                        text = f"Motif transfer: Hub '{c1}' ({d1}) may inspire solutions in '{c2}' ({d2}) (hub_sim={sim:.3f})"
                        insights.append({"insight": text, "confidence": float(quality), "strategy": "motif_transfer", "rationale": f"hub_sim={sim:.3f}", "tags": ["motif", "transfer", "graph"]})
        except Exception as e:
            logger.debug("Motif transfer failed: %s", e)
        return insights

    def _strategy_neural_fusion(self, n_samples: int = 4) -> List[Dict[str, Any]]:
        """
        Use the generator pipeline (if available) to produce human-readable fusion hypotheses
        given two concepts and their retrieval contexts (RAG-ish).
        """
        insights = []
        if self.generator is None:
            return insights
        # sample random cross-domain pairs
        keys = self.faiss_meta
        if not keys:
            return insights
        pairs = []
        for _ in range(n_samples):
            a = random.choice(keys)
            b = random.choice(keys)
            if a[0] != b[0]:
                pairs.append((a, b))
        for (d1, c1), (d2, c2) in pairs:
            prompt = f"Combine these two scientific concepts into a concise research idea:\n1) {c1} ({d1})\n2) {c2} ({d2})\nGive a 2-3 sentence hypothesis and one practical experiment or implementation. Be cautious and non-harmful."
            try:
                out = self.generator(prompt, max_length=180, num_return_sequences=1)
                text = out[0].get("generated_text", "").strip()
                # score fusion by novelty heuristics (distance of embeddings)
                emb1 = self.embeddings_cache.get(("concept", c1)) or self._encode_concepts([c1])[0]
                emb2 = self.embeddings_cache.get(("concept", c2)) or self._encode_concepts([c2])[0]
                novelty = 1.0 - _safe_cosine(emb1, emb2)
                quality = max(0.3, min(0.99, 0.45 + 0.5 * novelty))
                insights.append({"insight": text, "confidence": float(quality), "strategy": "neural_fusion", "rationale": f"novelty={novelty:.3f}", "tags": ["fusion", "generator"]})
            except Exception as e:
                logger.debug("Generator failed on fusion pair (%s,%s): %s", c1, c2, e)
        return insights

    # -------------------------
    # Utilities
    # -------------------------
    def _safe_text_similarity(self, a: str, b: str) -> float:
        """Proxy for text similarity used for deduplication."""
        return _safe_text_similarity(a, b)

    # Expose as helper in module scope to avoid recursion
def _safe_text_similarity(a: str, b: str) -> float:
    """Lightweight mix of token overlap and LCS normalized length."""
    try:
        # quick embedding-based similarity if both present in cache and model available
        # try to use embeddings if available
        # (we cannot access self here, so rely on simple heuristics)
        ta = set(a.lower().split())
        tb = set(b.lower().split())
        if not ta or not tb:
            return 0.0
        j = len(ta & tb) / len(ta | tb)
        lcs = _longest_common_substring(a, b)
        lcs_score = len(lcs) / max(len(a), len(b), 1)
        return 0.6 * j + 0.4 * lcs_score
    except Exception:
        return 0.0


def _longest_common_substring(a: str, b: str) -> str:
    if not a or not b:
        return ""
    la, lb = len(a), len(b)
    table = [[0] * (lb + 1) for _ in range(la + 1)]
    longest, x_longest = 0, 0
    for i in range(1, la + 1):
        for j in range(1, lb + 1):
            if a[i - 1] == b[j - 1]:
                table[i][j] = table[i - 1][j - 1] + 1
                if table[i][j] > longest:
                    longest = table[i][j]
                    x_longest = i
    return a[x_longest - longest : x_longest]


# -------------------------
# Lightweight manual test
# -------------------------
if __name__ == "__main__":
    logging.getLogger().setLevel(logging.DEBUG)
    synth = AdvancedKnowledgeSynthesizer(
        name="PrometheusUltraX100",
        embedding_model_name="all-MiniLM-L6-v2",
        generation_model_name=None,  # set to a small model (e.g., gpt2) only if you want generator
        use_faiss=True,
        persist_dir="knowledge_store_tmp",
        device="cpu",
        max_workers=4,
        theatrical_mode=False,
        safe_mode=True,
        random_seed=1234,
    )

    scientific_knowledge = {
        "physics": ["quantum entanglement", "relativity", "thermodynamics", "electromagnetism", "superconductivity"],
        "biology": ["evolution", "DNA replication", "cellular respiration", "neural networks", "enzyme catalysis"],
        "computer_science": ["algorithms", "machine learning", "neural networks", "optimization", "data structures"],
        "mathematics": ["calculus", "probability theory", "graph theory", "linear algebra", "complex systems"],
        "engineering": ["systems design", "materials science", "energy efficiency", "control theory", "signal processing"],
        "chemistry": ["organic synthesis", "catalysis", "electrochemistry"],
    }

    # Ingest knowledge in bulk
    for domain, concepts in scientific_knowledge.items():
        synth.ingest_knowledge(domain, concepts, persist=False, parallel=True)

    logger.info("Knowledge ingestion complete. Domains: %s", list(synth.knowledge_base.keys()))
    insights = synth.generate_cross_domain_insights(top_k=10, probes=20)
    logger.info("Top Insights:")
    for i, ins in enumerate(insights, 1):
        logger.info("%02d) %s (conf=%.3f) [%s]", i, ins["insight"], ins.get("confidence", 0.0), ins.get("strategy"))

    # Persist final state
    synth.persist_state()
